---
---

@inproceedings{ramesh-etal-2023-comparative,
    title = "A Comparative Study on the Impact of Model Compression Techniques on Fairness in Language Models",
    author = "Ramesh, Krithika  and
      Chavan*, Arnav  and
      Pandit*, Shrey  and
      Sitaram, Sunayana",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-long.878",
    pages = "15762--15782",
    abstract = "Compression techniques for deep learning have become increasingly popular, particularly in settings where latency and memory constraints are imposed. Several methods, such as pruning, distillation, and quantization, have been adopted for compressing models, each providing distinct advantages. However, existing literature demonstrates that compressing deep learning models could affect their fairness. Our analysis involves a comprehensive evaluation of pruned, distilled, and quantized language models, which we benchmark across a range of intrinsic and extrinsic metrics for measuring bias in text classification. We also investigate the impact of using multilingual models and evaluation measures. Our findings highlight the significance of considering both the pre-trained model and the chosen compression strategy in developing equitable language technologies. The results also indicate that compression strategies can have an adverse effect on fairness measures.",
}

@inproceedings{gupta2023end-to-end,
author = {Gupta, Gauri and Ramesh, Krithika and Bhattacharya, Anwesh and Gupta, Divya and Sharma, Rahul and Chandran, Nishanth and Sen, Rijurekha},
title = {End-to-end Privacy Preserving Training and Inference for Air Pollution Forecasting with Data from Rival Fleets},
booktitle = {Privacy Enhancing technologies Symposium (PETS) 2023},
year = {2023},
month = {July},
abstract = {Privacy-preserving machine learning (PPML) promises to train machine learning (ML) models by combining data spread across multiple data silos. Theoretically, secure multiparty computation (MPC) allows multiple data owners to train models on their joint data without revealing the data to each other. However, the prior implementations of this secure training using MPC have three limitations: they have only been evaluated on CNNs, and LSTMs have been ignored; fixed point approximations have affected training accuracies compared to training in floating point; and due to significant latency overheads of secure training via MPC, its relevance for practical tasks with streaming data remains unclear.

The motivation of this work is to report our experience of addressing the practical problem of secure training and inference of models for urban sensing problems, e.g., traffic congestion estimation, or air pollution monitoring in large cities, where data can be contributed by rival fleet companies while balancing the privacy-accuracy trade-offs using MPC-based techniques.

Our first contribution is to design a custom ML model for this task that can be efficiently trained with MPC within a desirable latency. In particular, we design a GCN-LSTM and securely train it on time-series sensor data for accurate forecasting, within 7 minutes per epoch. As our second contribution, we build an end-to-end system of private training and inference that provably matches the training accuracy of cleartext ML training. This work is the first to securely train a model with LSTM cells. Third, this trained model is kept secret-shared between the fleet companies and allows clients to make sensitive queries to this model while carefully handling potentially invalid queries. Our custom protocols allow clients to query predictions from privately trained models in milliseconds, all the while maintaining accuracy and cryptographic security.},
url = {https://www.microsoft.com/en-us/research/publication/end-to-end-privacy-preserving-training-and-inference-for-air-pollution-forecasting-with-data-from-rival-fleets/},
}

@inproceedings{ahuja2023mega,
      title={MEGA: Multilingual Evaluation of Generative AI}, 
      author={Kabir Ahuja and Harshita Diddee and Rishav Hada and Millicent Ochieng and Krithika Ramesh and Prachi Jain and Akshay Nambi and Tanuja Ganu and Sameer Segal and Maxamed Axmed and Kalika Bali and Sunayana Sitaram},
      booktitle = {Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
      year={2023},
      abstract = {Generative AI models have shown impressive performance on many Natural Language Processing tasks such as language understanding, reasoning, and language generation. An important question being asked by the AI community today is about the capabilities and limits of these models, and it is clear that evaluating generative AI is very challenging. Most studies on generative LLMs have been restricted to English and it is unclear how capable these models are at understanding and generating text in other languages. We present the first comprehensive benchmarking of generative LLMs - MEGA, which evaluates models on standard NLP benchmarks, covering 16 NLP datasets across 70 typologically diverse languages. We compare the performance of generative LLMs including Chat-GPT and GPT-4 to State of the Art (SOTA) non-autoregressive models on these tasks to determine how well generative models perform compared to the previous generation of LLMs. We present a thorough analysis of the performance of models across languages and tasks and discuss challenges in improving the performance of generative LLMs on low-resource languages. We create a framework for evaluating generative LLMs in the multilingual setting and provide directions for future progress in the field.},
      url={https://aclanthology.org/2023.emnlp-main.258}
}

@article{Ramesh2023FairnessIL,
  title={Fairness in Language Models Beyond English: Gaps and Challenges},
  author={Krithika Ramesh and Sunayana Sitaram and Monojit Choudhury},
  journal={Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics (Findings)},
  year={2023},
  abstract="With language models becoming increasingly ubiquitous, it has become essential to address their inequitable treatment of diverse demographic groups and factors. Most research on evaluating and mitigating fairness harms has been concentrated on English, while multilingual models and non-English languages have received comparatively little attention. This paper presents a survey of fairness in multilingual and non-English contexts, highlighting the shortcomings of current research and the difficulties faced by methods designed for English. We contend that the multitude of diverse cultures and languages across the world makes it infeasible to achieve comprehensive coverage in terms of constructing fairness datasets. Thus, the measurement and mitigation of biases must evolve beyond the current dataset-driven practices that are narrowly focused on specific dimensions and types of biases and, therefore, impossible to scale across languages and cultures.",
  url={https://arxiv.org/abs/2302.12578},
  selected = {true}
}

@inproceedings{ramesh-etal-2022-revisiting,
    title = "Revisiting Queer Minorities in Lexicons",
    author = "Ramesh, Krithika  and
      Kumar, Sumeet  and
      Khudabukhsh, Ashiqur",
    booktitle = "Proceedings of the Sixth Workshop on Online Abuse and Harms (WOAH)",
    month = jul,
    year = "2022",
    address = "Seattle, Washington (Hybrid)",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.woah-1.23",
    doi = "10.18653/v1/2022.woah-1.23",
    pages = "245--251",
    abstract = "Lexicons play an important role in content moderation often being the first line of defense. However, little or no literature exists in analyzing the representation of queer-related words in them. In this paper, we consider twelve well-known lexicons containing inappropriate words and analyze how gender and sexual minorities are represented in these lexicons. Our analyses reveal that several of these lexicons barely make any distinction between pejorative and non-pejorative queer-related words. We express concern that such unfettered usage of non-pejorative queer-related words may impact queer presence in mainstream discourse. Our analyses further reveal that the lexicons have poor overlap in queer-related words. We finally present a quantifiable measure of consistency and show that several of these lexicons are not consistent in how they include (or omit) queer-related words.",
    selected = {true}
}

@article{Ramesh_KhudaBukhsh_Kumar_2022, 
title={‘Beach’ to ‘Bitch’: Inadvertent Unsafe Transcription of Kids’ Content on YouTube}, 
url={https://ojs.aaai.org/index.php/AAAI/article/view/21470}, 
abstract={Over the last few years, YouTube Kids has emerged as one of the highly competitive alternatives to television for children’s entertainment. Consequently, YouTube Kids’ content should receive an additional level of scrutiny to ensure children’s safety. While research on detecting offensive or inappropriate content for kids is gaining momentum, little or no current work exists that investigates to what extent AI applications can (accidentally) introduce content that is inappropriate for kids. In this paper, we present a novel (and troubling) finding that well-known automatic speech recognition (ASR) systems may produce text content highly inappropriate for kids while transcribing YouTube Kids’ videos. We dub this phenomenon as inappropriate content hallucination. Our analyses suggest that such hallucinations are far from occasional, and the ASR systems often produce them with high confidence. We release a first-of-its-kind data set of audios for which the existing state-of-the-art ASR systems hallucinate inappropriate content for kids. In addition, we demonstrate that some of these errors can be fixed using language models.}, 
journal={Proceedings of the AAAI Conference on Artificial Intelligence, AI for Social Impact Track}, 
author={Ramesh, Krithika and KhudaBukhsh, Ashiqur R. and Kumar, Sumeet}, 
year={2022}, month={Jun.}, pages={12108-12118}},
selected = {true}
}

@article{Naidu2021TowardsQT,
  title={Towards Quantifying the Carbon Emissions of Differentially Private Machine Learning},
  author={Rakshit Naidu* and Harshita Diddee* and Ajinkya Mulay* and Aleti Vardhan* and Krithika Ramesh* and Ahmed S. Zamzam},
  journal={Socially Responsible Machine Learning Workshop, ICML, 2021},
  year={2021},
  abstract="In recent years, machine learning techniques utilizing large scale datasets have achieved remarkable performance. Differential privacy, by means of adding noise, provides strong privacy guarantees for such learning algorithms. The cost of differential privacy is often a reduced model accuracy and a lowered convergence speed. This paper investigates the impact of differential privacy on learning algorithms in terms of their carbon footprint due to either longer run-times or failed experiments. Through extensive experiments, further guidance is provided on choosing the noise levels which can strike a balance between desired privacy levels and reduced carbon emissions",
  url={https://arxiv.org/abs/2107.06946}
}

@inproceedings{ramesh-etal-2021-evaluating,
    title = "Evaluating Gender Bias in {H}indi-{E}nglish Machine Translation",
    author = "Gupta*, Gauri and
    Ramesh*, Krithika  and
    Singh, Sanjay",
    booktitle = "Proceedings of the 3rd Workshop on Gender Bias in Natural Language Processing",
    month = aug,
    year = "2021",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.gebnlp-1.3",
    pages = "16--23",
    abstract = "With language models being deployed increasingly in the real world, it is essential to address the issue of the fairness of their outputs. The word embedding representations of these language models often implicitly draw unwanted associations that form a social bias within the model. The nature of gendered languages like Hindi, poses an additional problem to the quantification and mitigation of bias, owing to the change in the form of the words in the sentence, based on the gender of the subject. Additionally, there is sparse work done in the realm of measuring and debiasing systems for Indic languages. In our work, we attempt to evaluate and quantify the gender bias within a Hindi-English machine translation system. We implement a modified version of the existing TGBI metric based on the grammatical considerations for Hindi. We also compare and contrast the resulting bias measurements across multiple metrics for pre-trained embeddings and the ones learned by our machine translation model.",
}