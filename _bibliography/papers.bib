---
---

@article{Ramesh2023FairnessIL,
  title={Fairness in Language Models Beyond English: Gaps and Challenges},
  author={Krithika Ramesh and Sunayana Sitaram and Monojit Choudhury},
  journal={Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics (Findings), 2023},
  year={2023},
  url={https://arxiv.org/abs/2302.12578},
  selected = {true}
}

@inproceedings{ramesh-etal-2022-revisiting,
    title = "Revisiting Queer Minorities in Lexicons",
    author = "Ramesh, Krithika  and
      Kumar, Sumeet  and
      Khudabukhsh, Ashiqur",
    booktitle = "Proceedings of the Sixth Workshop on Online Abuse and Harms (WOAH)",
    month = jul,
    year = "2022",
    address = "Seattle, Washington (Hybrid)",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.woah-1.23",
    doi = "10.18653/v1/2022.woah-1.23",
    pages = "245--251",
    abstract = "Lexicons play an important role in content moderation often being the first line of defense. However, little or no literature exists in analyzing the representation of queer-related words in them. In this paper, we consider twelve well-known lexicons containing inappropriate words and analyze how gender and sexual minorities are represented in these lexicons. Our analyses reveal that several of these lexicons barely make any distinction between pejorative and non-pejorative queer-related words. We express concern that such unfettered usage of non-pejorative queer-related words may impact queer presence in mainstream discourse. Our analyses further reveal that the lexicons have poor overlap in queer-related words. We finally present a quantifiable measure of consistency and show that several of these lexicons are not consistent in how they include (or omit) queer-related words.",
    selected = {true}
}

@article{Ramesh_KhudaBukhsh_Kumar_2022, 
title={‘Beach’ to ‘Bitch’: Inadvertent Unsafe Transcription of Kids’ Content on YouTube}, 
volume={36}, 
url={https://ojs.aaai.org/index.php/AAAI/article/view/21470}, 
DOI={10.1609/aaai.v36i11.21470}, 
abstractNote={Over the last few years, YouTube Kids has emerged as one of the highly competitive alternatives to television for children’s entertainment. Consequently, YouTube Kids’ content should receive an additional level of scrutiny to ensure children’s safety. While research on detecting offensive or inappropriate content for kids is gaining momentum, little or no current work exists that investigates to what extent AI applications can (accidentally) introduce content that is inappropriate for kids. In this paper, we present a novel (and troubling) finding that well-known automatic speech recognition (ASR) systems may produce text content highly inappropriate for kids while transcribing YouTube Kids’ videos. We dub this phenomenon as inappropriate content hallucination. Our analyses suggest that such hallucinations are far from occasional, and the ASR systems often produce them with high confidence. We release a first-of-its-kind data set of audios for which the existing state-of-the-art ASR systems hallucinate inappropriate content for kids. In addition, we demonstrate that some of these errors can be fixed using language models.}, 
number={11}, 
journal={Proceedings of the AAAI Conference on Artificial Intelligence}, 
author={Ramesh, Krithika and KhudaBukhsh, Ashiqur R. and Kumar, Sumeet}, 
year={2022}, month={Jun.}, pages={12108-12118}},
selected = {true}
}

@article{Naidu2021TowardsQT,
  title={Towards Quantifying the Carbon Emissions of Differentially Private Machine Learning},
  author={Rakshit Naidu and Harshita Diddee and Ajinkya Mulay and Aleti Vardhan and Krithika Ramesh and Ahmed S. Zamzam},
  journal={Socially Responsible Maching Learning Workshop, ICML, 2021},
  year={2021},
  url={https://arxiv.org/abs/2107.06946}
}

@inproceedings{ramesh-etal-2021-evaluating,
    title = "Evaluating Gender Bias in {H}indi-{E}nglish Machine Translation",
    author = "Ramesh, Krithika  and
      Gupta, Gauri  and
      Singh, Sanjay",
    booktitle = "Proceedings of the 3rd Workshop on Gender Bias in Natural Language Processing",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.gebnlp-1.3",
    doi = "10.18653/v1/2021.gebnlp-1.3",
    pages = "16--23",
    abstract = "With language models being deployed increasingly in the real world, it is essential to address the issue of the fairness of their outputs. The word embedding representations of these language models often implicitly draw unwanted associations that form a social bias within the model. The nature of gendered languages like Hindi, poses an additional problem to the quantification and mitigation of bias, owing to the change in the form of the words in the sentence, based on the gender of the subject. Additionally, there is sparse work done in the realm of measuring and debiasing systems for Indic languages. In our work, we attempt to evaluate and quantify the gender bias within a Hindi-English machine translation system. We implement a modified version of the existing TGBI metric based on the grammatical considerations for Hindi. We also compare and contrast the resulting bias measurements across multiple metrics for pre-trained embeddings and the ones learned by our machine translation model.",
}